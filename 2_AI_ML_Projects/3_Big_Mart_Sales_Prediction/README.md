# Retail Sales Prediction - Regression Models

A regression-based machine learning project to predict outlet sales using the BigMart dataset. This repository explores multiple linear and non-linear regression algorithms with detailed evaluation, visualization, and model tuning.

---

## Project Summary

We aim to predict **Item_Outlet_Sales** based on a variety of item and outlet features using several regression models. Our goal is to build a robust, interpretable, and accurate model that generalizes well to unseen data.

---

## ðŸ”§ Models Used & Performance Summary

| Model                         | CV RÂ²   | Test RÂ² | Test RMSE | Test MAE |
|------------------------------|---------|---------|-----------|----------|
| Polynomial Regression (d=2)  | **0.5900** | **0.5778** | 1141.65    | **815.78** |
| Random Forest                | 0.5581  | 0.5522  | 1175.76   | 824.17   |
| XGBoost                      | 0.5149  | 0.5017  | 1240.25   | 866.26   |
| Linear, Ridge, Lasso, Bayes  | ~0.504  | ~0.489  | 1255.82   | 945.00+  |
| SVR                          | 0.0329  | 0.0343  | 1647.43   | Very High|

âœ… **Polynomial Regression** achieved the best generalization performance.

---

## Key Insights

- **Polynomial Regression (degree 2)** outperformed all other models by effectively capturing non-linear interactions.
- **Tree-based models (Random Forest & XGBoost)** overfit the training data. Regularization needed.
- **SVR severely underfit**, failing to capture signal in data.
- **Linear models (Ridge, Lasso, Bayesian Ridge)** serve as stable baselines.

---

## Recommendations & Next Steps

### âœ… Immediate Fixes
- **Transform Skewed Features** (`Item_Visibility`, `Item_MRP`) using `log` or `Box-Cox`.
- **Handle Outliers** in `Item_Visibility` and `Item_Outlet_Sales` using capping or trimming.
- **Regularize Trees**: Tune `max_depth`, `min_samples_leaf`, and XGBoostâ€™s `eta`, `subsample`.

### ðŸ”„ Iterative Improvements
| Step | Goal | Metric |
|------|------|--------|
| Re-train with transforms | Reduce skew | Î” CV RÂ² â‰¥ +0.02 |
| Outlier cleaning | Lower error variance | MAE â†“ 5â€“10% |
| Hyperparameter tuning | Prevent overfitting | Train RÂ² â€“ CV RÂ² gap â‰¤ 0.10 |
| Robust CV | Ensure stability | CV RÂ² std dev < Â±0.02 |
| Try new models | Benchmarking | RMSE/RÂ² improvements |

---

## Tech Stack

- Python (Pandas, NumPy, Scikit-Learn, XGBoost, Seaborn, Matplotlib)
- Jupyter Notebooks
- Regression models: Linear, Ridge, Lasso, Bayesian Ridge, Polynomial, SVR, Random Forest, XGBoost

---

## Future Scope

- Implement SHAP for model explainability
- Feature engineering (interactions, encodings)
- Ensemble stacking
- Incorporate external features (e.g., region-based sales trends)

---

## ðŸ“ Author

**Udaybhan Singh Rana**  
ðŸ”— [LinkedIn](https://www.linkedin.com/in/udaybhan-rana/)

---